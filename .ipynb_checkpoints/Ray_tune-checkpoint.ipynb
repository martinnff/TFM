{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "organizational-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from os.path import exists\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "import scipy.sparse as scpy\n",
    "from torch.nn import Linear\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.data as data\n",
    "import torch_geometric.data.data as Batch\n",
    "import torch.utils.data as data_utils\n",
    "import torch_geometric.transforms as T\n",
    "from torch.nn.init import xavier_uniform\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import torch_geometric.datasets as datasets\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch_geometric.transforms as transforms\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch.nn.utils import clip_grad_norm_, clip_grad_value_\n",
    "from torch_geometric.nn import  GATv2Conv, GraphNorm,  SAGEConv, global_mean_pool, global_max_pool\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sharp-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sp(path,seed=1,split=0.9,parcela=0):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    parcelas = os.listdir(path)\n",
    "    n_sample = round(len(parcelas)*split)\n",
    "    train_ind = np.random.choice(np.linspace(1,len(parcelas),len(parcelas)),n_sample,replace=False).astype(int)\n",
    "    test_ind = np.delete(np.linspace(1,len(parcelas),len(parcelas)),train_ind-1).astype(int)\n",
    "    test_graphs = []\n",
    "    train_graphs = []\n",
    "    \n",
    "    parcela = parcela \n",
    "    for itrain in train_ind:\n",
    "        filename = f\"{path}/parcela{itrain}\"\n",
    "        archivos = os.listdir(filename)\n",
    "        ngrafos=int(len(archivos)/4)\n",
    "        parcela+=1\n",
    "        for i in range(ngrafos):\n",
    "            i+=1\n",
    "            edges = pd.read_csv(f\"{filename}/el{i}.csv\").iloc[:,1:]\n",
    "            attributes = pd.read_csv(f\"{filename}/z{i}.csv\").iloc[:,1]\n",
    "            label = pd.read_csv(f\"{filename}/y{i}.csv\").iloc[:,1]\n",
    "            weights = pd.read_csv(f\"{filename}/ea{i}.csv\").iloc[:,1]\n",
    "            weights = torch.tensor(weights.to_numpy(),dtype=torch.float)\n",
    "            edge_idx = torch.tensor(edges.to_numpy().transpose(), dtype=torch.long)\n",
    "            edge_idx -= 1\n",
    "            attrs = torch.tensor(attributes.to_numpy(), dtype=torch.float)\n",
    "            np_lab = label.to_numpy()\n",
    "            y = torch.tensor(np_lab, dtype=torch.long)-1\n",
    "            y = y[0]\n",
    "            graph = Data(x=attrs, edge_index=edge_idx,  y=y, edge_attr = weights,parcela=parcela)\n",
    "            train_graphs.append(graph)     \n",
    "    for itest in test_ind:\n",
    "        filename = f\"{path}/parcela{itest}\"\n",
    "        archivos = os.listdir(filename)\n",
    "        ngrafos=int(len(archivos)/4)\n",
    "        parcela+=1\n",
    "        for i in range(ngrafos):\n",
    "            i+=1\n",
    "            edges = pd.read_csv(f\"{filename}/el{i}.csv\").iloc[:,1:]\n",
    "            attributes = pd.read_csv(f\"{filename}/z{i}.csv\").iloc[:,1]\n",
    "            label = pd.read_csv(f\"{filename}/y{i}.csv\").iloc[:,1]\n",
    "            weights = pd.read_csv(f\"{filename}/ea{i}.csv\").iloc[:,1]\n",
    "            weights = torch.tensor(weights.to_numpy(),dtype=torch.float)\n",
    "            edge_idx = torch.tensor(edges.to_numpy().transpose(), dtype=torch.long)\n",
    "            edge_idx -= 1\n",
    "            attrs = torch.tensor(attributes.to_numpy(), dtype=torch.float)\n",
    "            np_lab = label.to_numpy()\n",
    "            y = torch.tensor(np_lab, dtype=torch.long)-1\n",
    "            y = y[0]\n",
    "            graph = Data(x=attrs, edge_index=edge_idx,  y=y, edge_attr = weights,parcela=parcela)\n",
    "            test_graphs.append(graph)\n",
    "    return [test_graphs,train_graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "valid-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos1 = split_sp(path='./datos/grafos/train/sp1',parcela=0)\n",
    "parcela=(len(datos1[0])+len(datos1[1]))\n",
    "datos2 = split_sp(path='./datos/grafos/train/sp2',parcela=parcela)\n",
    "parcela=parcela+(len(datos2[0])+len(datos2[1]))\n",
    "datos3 = split_sp(path='./datos/grafos/train/sp3',parcela=parcela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nasty-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test.extend(datos1[0])\n",
    "test.extend(datos2[0])\n",
    "test.extend(datos3[0])\n",
    "\n",
    "train = []\n",
    "train.extend(datos1[1])\n",
    "train.extend(datos2[1])\n",
    "train.extend(datos3[1])\n",
    "\n",
    "with open(\"./datos/train_graphs1_90.pkl\",'wb') as f:\n",
    "    pickle.dump(train,f)\n",
    "with open(\"./datos/test_graphs1_90.pkl\",'wb') as f:\n",
    "    pickle.dump(test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "optical-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(InMemoryDataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.data_list = []\n",
    "        with open(path,'rb') as f:\n",
    "            self.data_list.extend(pickle.load(f))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "approximate-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphDataset(\"./datos/train_graphs1_90.pkl\")\n",
    "test_dataset = GraphDataset(\"./datos/test_graphs1_90.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quantitative-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# funcion para inicializar pesos de la red\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, nonlinearity='relu')\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "dp=0.2\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hid = 64, \n",
    "                 in_head = 16, \n",
    "                 in_features = 1,\n",
    "                 out_features = 4,\n",
    "                 s_fc1 = 2048,\n",
    "                 s_fc2 = 1024):\n",
    "        super(GAT, self).__init__()\n",
    "        \n",
    "        self.hid = hid\n",
    "        self.in_head = in_head\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s_fc1 = s_fc1\n",
    "        self.s_fc2 = s_fc2\n",
    "        \n",
    "        self.conv1 =  GATv2Conv(self.in_features, self.out_features,edge_dim=1,heads=self.in_head,concat=True)\n",
    "        self.conv2 =  SAGEConv(self.out_features*self.in_head, self.hid,normalize=False)\n",
    "        self.norm1=GraphNorm(self.out_features*self.in_head)\n",
    "        self.fc1 = nn.Linear(self.hid*2,self.s_fc1)\n",
    "        self.fc2 = nn.Linear(self.s_fc1,self.s_fc2)\n",
    "        self.fc3 = nn.Linear(self.s_fc2,3)\n",
    "        self.conv1.apply(init_weights)\n",
    "        self.conv2.apply(init_weights)\n",
    "        self.fc1.apply(init_weights)\n",
    "        self.fc2.apply(init_weights)\n",
    "        self.fc3.apply(init_weights)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        edge_attr = torch.unsqueeze(edge_attr,1)\n",
    "        x = torch.unsqueeze(x,-1)\n",
    "        \n",
    "        x = self.conv1(x,edge_index,edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.norm1(x,batch)\n",
    "        x = F.dropout(x, p=dp, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=dp, training=self.training)\n",
    "        \n",
    "        x1 = global_max_pool(x,batch)\n",
    "        x2 = global_mean_pool(x,batch)\n",
    "        x = torch.cat((x1,x2),1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.dropout(x, p=dp, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=dp, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "talented-terrorist",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3692/278011564.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = GAT(config['hid'],\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'in_head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'in_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's_fc1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = GAT(config['hid'],\n",
    "            config['in_head'],\n",
    "            config['in_features'],\n",
    "            config['out_features'],\n",
    "            config['s_fc1'],\n",
    "            config['s_fc2'])\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['wd'])\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hungarian-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graphs(config, checkpoint_dir=None, data_dir=None):\n",
    "    model = GAT(config['hid'],\n",
    "                config['in_head'],\n",
    "                config['in_features'],\n",
    "                config['out_features'],\n",
    "                config['s_fc1'],\n",
    "                config['s_fc2'])\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['wd'])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    model.to(device)\n",
    "    criterion=torch.nn.CrossEntropyLoss(torch.tensor([1.1,1.05,1.0]).to(device))\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    trainset, testset = train_dataset, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=8)\n",
    "    count=0\n",
    "\n",
    "    for epoch in range(50):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        lss=0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            batch=data.to(device)\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            running_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            if(idx!=0 and idx%5==0):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),2)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            count+=1\n",
    "            \n",
    "\n",
    "        running_loss/=count\n",
    "        epoch_steps += 1\n",
    "\n",
    "        print('[Epoch %4d/%4d] Loss: % 2.2e' % (epoch + 1, 50, running_loss))\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                data = data.to(device)\n",
    "\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, data.y)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "coordinated-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, device=\"cpu\"):\n",
    "\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=4, shuffle=False, drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            data.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == data.y).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "closed-chile",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3692/828601467.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     checkpoint_at_end=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    \"hid\": tune.choice([4, 8, 16,32, 64,88,128]),\n",
    "    \"in_head\": tune.choice([2, 4, 8, 16,20, 24]),\n",
    "    \"in_features\": 1,\n",
    "    \"out_features\": tune.choice([2, 4, 6, 8,12,16,32]),\n",
    "    \"s_fc1\": tune.choice([256, 512, 1024, 2048,3096]),\n",
    "    \"s_fc2\": tune.choice([256, 512, 1024, 2048,3096]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"wd\": tune.loguniform(5e-4, 1e-6)\n",
    "}\n",
    "\n",
    "gpus_per_trial = 1\n",
    "# ...\n",
    "result = tune.run(\n",
    "    partial(train_graphs),\n",
    "    resources_per_trial={\"cpu\": 4, \"gpu\": gpus_per_trial},\n",
    "    config=config,\n",
    "    num_samples=1,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-anthropology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local1",
   "language": "python",
   "name": "local1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
